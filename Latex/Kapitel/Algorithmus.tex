%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beschreibung des Algorithmus und nötigen Anppassungen %%%%%%%%%%%%%%%%%%%%%%
\chapter{Algorithmus}
\label{chap:algorithmus}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erweiterung für Second Order Cone Problems}%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungSOCP}
Um mit dem effizienten Algorithmus aus \cite{BOY10} auch kompliziertere/komplexere [TODO] Probleme, hier speziell Second Order Cone Problems (SOCP\abk{SOCP}{Second Order Cone Problem}) zu lösen, wurde der Algorithmus wie gleich folgt erweitert. Dabei wurde speziell darauf geachtet nicht die Struktur der entstehenden Matrizen zu verändern, sodass diese auch weiterhin ausgenutzt werden kann. Allerdings sind für ein SOCP nun die Matrizen für die Ungleichungsnebenbedingungen nicht mehr konstant sondern hängen von $x(k)$ ab, sodass sie in jedem MPC Schritt angepasst werden müssen, was einen erhöhten Rechenaufwand bedeutet.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SOCP Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:SOCPFormulierung}
Zusäliche Ungleichungsnebenbedingung sieht wie folgt aus [TODO: woher \cite{BOY04}]:
\begin{equation}
 \label{eq:SOCPUNB}
 \left \|Ax+b  \right \|_{2} \leq c^{T}x+d
\end{equation}
Als generalized inequality nimmt Gleichung \ref{eq:SOCPUNB} leicht andere Form an:
\begin{equation}
 \left \|Ax+b  \right \|_{2}^2 \leq \left (c^{T}x+d  \right )^{2}
\end{equation}
Im folgenden lässt sich die Ungleichungsnebenbedingung so leichter umformen und ableiten, speziell hebt sich so später ein Wurzelterm auf. Mit $x=z$ ergeben sich die zusätzlichen $j$ Funktionen für die logarithmic barrier function somit zu
\begin{equation}
 -f_{j}\left (x  \right )=\left (c_{j}^{T}x+d_{j} \right )^{2}-\left \|A_{j}x+b_{j}  \right \|_{2}^2
\end{equation}
Alle $k$ barrier function Funktionen lassen sich zu
\begin{equation}
 -f_{k}\left (z  \right )
 = -\begin{bmatrix}f_{i}\\f_{j}\end{bmatrix}_{k}
 =\begin{bmatrix}h_{i}\\0\end{bmatrix}_{k} - \begin{bmatrix}p_{i}\\
 \left (\left\|A_{j}z+b_{j}\right \|_{2}^{2} - \left(c_{j}^{T}z+d_{j}\right )^{2} \right )z^{-1} \end{bmatrix}_{k}z
\end{equation}
zusammenfassen. Für den Algorithmus wird nun weiterhin die Ableitung (Gradient und Hessian) der logarithmic barrier function $\phi(z)$ benötigt, die aus unter anderem $\nabla f_{k}(z)$ und $\nabla^{2} f_{k}(z)$ gebildet werden.
\begin{equation}
 \nabla f_{k}\left (z  \right )=\begin{bmatrix}p_{i}\\ -2\left(\left(c_{j}^{T}z + d_{j}\right )c_{j} - A_{j}^{T} \left(A_{j}z + b_{j} \right )\right)\end{bmatrix}_{k}
\end{equation}
\begin{equation}
 \nabla^{2} f_{k}\left (z  \right )=\begin{bmatrix}0\\ -2 \left( c_{j}^{T}c_{j} - A_{j}^{T} A_{j}  \right)\end{bmatrix}_{k}
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Anpassung für test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:testcases}
Der implementierte Algorithmus wie in [Paper:, Section:] beschrieben kann auch verwendet werden, um Optimierungsprobleme zu lösen, die ihren Ursprung nicht in der Anwendung von MPC haben. Dazu sind keine wirklichen Anpassungen des Algorithmus notwendig. Da der Algorithmus allerdings die Struktur der bei MPC auftretenden Matrizen ausnutzt, muss der jeweilige test case so ``transformiert'' werden, dass dieser eine ähnliche Struktur aufweist.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Allgemeine Beschreibung der test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:beschreibungtestcases}
Nach \cite{MAR97} haben die test cases folgende Form:
\begin{equation}\begin{split}
  \label{eq:mar}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{A} \hat{x} = \hat{b}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Aber es existieren auch test cases mit weiteren Ungleichungsnebenbedingung der Form:
\begin{equation}
  \label{eq:mar_unb}
  \hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}
\end{equation}
Vereinheitlicht für \ref{eq:mar} und \ref{eq:mar_unb} schreiben
\begin{equation}\begin{split}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Wobei sich für 
\begin{equation*}
  \hat{b} = \hat{b}_{lower} = \hat{b}_{upper}
\end{equation*}
die Gleichungsnebenbedingungen
\begin{equation*}
  \hat{A} \hat{x} = \hat{b}
\end{equation*}
ergeben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithmus mit Prädiktionshorizont gleich eins}%%%%%%%%%%%%%%%%%%
\label{sec:algorithust1}
Um die test cases lösen zu können, muss der Prädiktionshorizont $T=1$ gewählt werden. Die Optimierungsvariable beschränkt sich damit auf
\begin{equation*}
  z=\left( u(t), x(t+T) \right)\in\mathbb{R}^{(m+n)}, \quad T=1
\end{equation*}
Die strukturierten Matrizen im Algorithmus zum lösen des Optimierungsproblems
\begin{equation*}\begin{split}
  \min \quad &z^{T}Hz+g^{T}z\\
  \text{s.t.} \quad &Pz\leq h, \quad Cz = b 
\end{split}\end{equation*}
reduzieren sich damit auf folgende Form:
\begin{equation*}\begin{split}
  H&=\begin{bmatrix}
  R & 0\\ 
  0 & Q_{f}
  \end{bmatrix}\\
  P&=\begin{bmatrix}
  F_{u} & 0\\ 
  0 & F_{f}
  \end{bmatrix}\\
  C&=\begin{bmatrix}
  -B & I
  \end{bmatrix}\\
  g&=\begin{bmatrix}
  r+2S^{T}x(t)\\ 
  q
  \end{bmatrix}\\
  h&=\begin{bmatrix}
  f-F_{x}x(t)\\ 
  f_{f}
  \end{bmatrix}\\
  b&=\begin{bmatrix}
  Ax(t)
  \end{bmatrix}\\
\end{split}\end{equation*}
Um den Algorithmus nun mit den test cases nach \cite{MAR97} zu testen muss
\begin{equation}
  H=\frac{1}{2}\hat{Q},\quad g=\hat{c}, \text{nicht korrekt, einzelne Untermatrizen setzen}
\end{equation}
gesetzt werden. Die Ungleichungsnebenbedingung
\begin{equation}
  F_{u}u(t)+F_{x}x(t)+F_{f}x(t+1) \leq f = f_{u}+f_{x}
\end{equation}
ergeben sich zu
\begin{equation*}\begin{split}
  F_{u}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{x}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{f}&=\begin{bmatrix}
  0 & 0
  \end{bmatrix}\\
  f&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}\\
  f_{f}&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}
\end{split}\end{equation*}
Als Gleichungsnebenbedingungen bleibt im implementierten Algorithmus
\begin{equation}
  x(t+1) = Ax(t)+Bu(t)
\end{equation}
Da allerdings x(t+1) auch zu dem Vektor der Optimierungsvariablen gehört muss
\begin{equation}
  \hat{b} = -Ax(t)\quad \text{mit} \quad A=-I
\end{equation}
gesetzt werden. Zusätzlich wird mit weiteren Ungleichungsnebenbedingung dafür gesorgt, dass $x(t+1)$ im Optimum nahe der Null liegt. Das bedeutet aber auch, dass in der Auswertung der Güte der eingehaltenen Gleichungsnebenbedingungen auch die Genauigkeit der zusätzlichen Ungleichungsnebenbedingung betrachtet werden muss.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%