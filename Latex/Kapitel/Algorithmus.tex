%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beschreibung des Algorithmus und nötigen Anppassungen %%%%%%%%%%%%%%%%%%%%%%
\chapter{Algorithmus}
\label{chap:algorithmus}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erweiterung für nichtlineare Constraints}%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungNonlincon}
Um mit dem effizienten Algorithmus aus \cite{BOY10} auch kompliziertere/komplexere [TODO] Probleme, in Kapitel \ref{sec:ErweiterungSOCP} Second Order Cone Problems (SOCP\abk{SOCP}{Second Order Cone Problem}), bzw in Kapitel \ref{sec:ErweiterungQCQP} Quadratic Programs/Problems mit Quadratic Constraints (QCQP\abk{QCQP}{Quadratic Constrained Quadratic Program}) zu lösen, wurde der Algorithmus wie gleich folgt erweitert. Dabei wurde speziell darauf geachtet nicht die Struktur der entstehenden Matrizen zu verändern, sodass diese auch weiterhin ausgenutzt werden kann. Allerdings sind für ein SOCP bzw QCQP nun die Matrizen für die Ungleichungsnebenbedingungen nicht mehr konstant sondern hängen von $z(k)$ ab, sodass sie in jedem MPC Schritt angepasst werden müssen, was einen erhöhten Rechenaufwand bedeutet.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Erweiterung für Quadratic Problems mit quadratic Constraints}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungQCQP}
Einführendes zu QCQPs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{QCQP-Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:QCQPFormulierung}
Zusäliche Ungleichungsnebenbedingung sieht wie folgt aus:
\begin{equation}
 x^{T}\Gamma x+\beta^{T}x\leq\alpha
\end{equation}
Mit $x=z$ kommen somit zu den $lT+l_{f}$ ursprünglichen mit den linearen Ungleichungsnebenbedingungen assoziierten Funktionen zusätzliche $p$ Funktionen für die logarithmic barrier function hinzu
\begin{equation}
 -f_{j}(z) = \alpha_{j} - z^{T}\Gamma_{j} z - \beta_{j}^{T}z, \quad j=1\dots p
\end{equation}
Für den Algorithmus wird nun weiterhin die Ableitung (Gradient und Hessian) der logarithmic barrier function $\phi(z)$ benötigt, die aus unter anderem $\nabla f_{k}(z)$ und $\nabla^{2} f_{k}(z)$ gebildet werden.
\begin{equation}
 \nabla f_{j}\left (z  \right )=2z^{T}\Gamma_{j}+\beta_{j}^{T}
\end{equation}
\begin{equation}
 \nabla^{2} f_{j}\left (z  \right )=2\Gamma_{j}
\end{equation}
Daraus ergibt sich der Gradient der barrier function $\phi(z)$ zu
\begin{equation}
 \nabla\phi\left ( z \right )=\sum_{k=1}^{lT+f_{f}+p}
 \frac{1}{\begin{bmatrix} h_{i}\\\alpha_{j} \end{bmatrix}_{k} -\begin{bmatrix} p_{i}^{T}\\\beta_{j}^{T}+z^{T}\Gamma_{j} \end{bmatrix}_{k}z}
 \begin{bmatrix} p_{i}^{T}\\\beta_{j}^{T}+2z^{T}\Gamma_{j} \end{bmatrix}_{k}
\end{equation}
Mit
\begin{equation}
 \hat{P}(z)=\begin{bmatrix} P\\ \beta_{1}^{T}+z^{T}\Gamma_{1}\\ \vdots\\ \beta_{p}^{T}+z^{T}\Gamma_{p} \end{bmatrix},\quad \hat{h} = \begin{bmatrix} h\\ \alpha_{1}^{T}\\ \vdots\\ \alpha_{p}^{T}\end{bmatrix}
\end{equation}
lässt sich das auch einfacher schreiben:
\begin{equation}
	\nabla\phi\left ( z \right )=\hat{P}^{T}(2z)\hat{d}
\end{equation}
Wobei
\begin{equation}
	\hat{d}_{k}=\frac{1}{\hat{h}_{k}-\hat{p}_{k}(z)z}
\end{equation}
und $\hat{p}_{k}(2z)$ die Zeilen in $\hat{P}(2z)$ sein sollen.
Um $\Phi$ zu bilden wird noch die zweite Ableitung von der barrier function benötigt
\begin{equation}
	\nabla^{2}\phi(z)=\hat{P}(2z)\text{diag}(\hat{d})^{2}\hat{P}(2z)+\sum_{j=1}^{p}\left (\hat{d}_{(lT+l_{f})+j}2\Gamma_{j}  \right )
\end{equation}
Die Berechnungen der Ableitungen haben also analog Struktur wie \cite{BOY10} bis auf den zusätzlichen Term, der bei der Hessian hinzukommt. Aber da dieser Term in der Implementierung nicht durch die Multiplikation einer großen Matrix $Gamma$ mit $z$ sonder identischer Teilmatrizen $Gamma_{k}$ mit den $T$ $x_{k}$ ist sicher gestellt das keine größeren Blöcke als $n\times n$ an den richtigen Stellen erzeugt werden. Das bedeutet, dass auch hier die Struktur der Matrix $\Phi$ nicht verändert wird.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Erweiterung für Second Order Cone Problems}%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungSOCP}
Bei der Ungleichungsnebenbedingungen für die Second Order Cone Constraints lässt sich die Berechnung leider nicht so Überschaubar darstellen, da hier Funktion und Ableitung nicht mehr so schön ähnlich sind. Die Anpassungen müssen daher wie folgt aussehen.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{SOCP Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:SOCPFormulierung}
Zusäliche Ungleichungsnebenbedingung sieht wie folgt aus [TODO: woher \cite{BOY04}]:
\begin{equation}
 \label{eq:SOCPUNB}
 \left \|Ax+b  \right \|_{2} \leq c^{T}x+d
\end{equation}
Als generalized inequality nimmt Gleichung \ref{eq:SOCPUNB} leicht andere Form an:
\begin{equation}
 \left \|Ax+b  \right \|_{2}^2 \leq \left (c^{T}x+d  \right )^{2}
\end{equation}
Im folgenden lässt sich die Ungleichungsnebenbedingung so leichter umformen und ableiten, speziell hebt sich so später ein Wurzelterm auf. Mit $x=z$ ergeben sich die zusätzlichen $j$ Funktionen für die logarithmic barrier function somit zu
\begin{equation}
 -f_{j}\left (x  \right )=\left (c_{j}^{T}x+d_{j} \right )^{2}-\left \|A_{j}x+b_{j}  \right \|_{2}^2
\end{equation}
Alle $k$ barrier function Funktionen lassen sich zu
\begin{equation}
 -f_{k}\left (z  \right )
 = -\begin{bmatrix}f_{i}\\f_{j}\end{bmatrix}_{k}
 =\begin{bmatrix}h_{i}\\0\end{bmatrix}_{k} - \begin{bmatrix}p_{i}^{T}z\\
 \left (\left\|A_{j}z+b_{j}\right \|_{2}^{2} - \left(c_{j}^{T}z+d_{j}\right )^{2} \right ) \end{bmatrix}_{k}
\end{equation}
zusammenfassen. Für den Algorithmus wird nun weiterhin die Ableitung (Gradient und Hessian) der logarithmic barrier function $\phi(z)$ benötigt, die aus unter anderem $\nabla f_{k}(z)$ und $\nabla^{2} f_{k}(z)$ gebildet werden.
\begin{equation}
 \nabla f_{k}\left (z  \right )=\begin{bmatrix}p_{i}^{T}\\ \left (-2\left(\left(c_{j}^{T}z + d_{j}\right )c_{j} - A_{j}^{T} \left(A_{j}z + b_{j} \right )\right) \right)^{T}\end{bmatrix}_{k}
\end{equation}
Das Transponieren der unteren Zeile ergibt sich dadurch, dass man zu $P$ eine Spalte anhängt also zu $P^{T}$ die transformierte dieser Spalte.
\begin{equation}
 \nabla^{2} f_{k}\left (z  \right )=\begin{bmatrix}0\\ -2 \left( c_{j}^{T}c_{j} - A_{j}^{T} A_{j}  \right)\end{bmatrix}_{k}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Anpassung für test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:testcases}
Der implementierte Algorithmus wie in [Paper:, Section:] beschrieben kann auch verwendet werden, um Optimierungsprobleme zu lösen, die ihren Ursprung nicht in der Anwendung von MPC haben. Dazu sind keine wirklichen Anpassungen des Algorithmus notwendig. Da der Algorithmus allerdings die Struktur der bei MPC auftretenden Matrizen ausnutzt, muss der jeweilige test case so ``transformiert'' werden, dass dieser eine ähnliche Struktur aufweist.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Allgemeine Beschreibung der test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:beschreibungtestcases}
Nach \cite{MAR97} haben die test cases folgende Form:
\begin{equation}\begin{split}
  \label{eq:mar}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{A} \hat{x} = \hat{b}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Aber es existieren auch test cases mit weiteren Ungleichungsnebenbedingung der Form:
\begin{equation}
  \label{eq:mar_unb}
  \hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}
\end{equation}
Vereinheitlicht für \ref{eq:mar} und \ref{eq:mar_unb} schreiben
\begin{equation}\begin{split}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Wobei sich für 
\begin{equation*}
  \hat{b} = \hat{b}_{lower} = \hat{b}_{upper}
\end{equation*}
die Gleichungsnebenbedingungen
\begin{equation*}
  \hat{A} \hat{x} = \hat{b}
\end{equation*}
ergeben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithmus mit Prädiktionshorizont gleich eins}%%%%%%%%%%%%%%%%%%
\label{sec:algorithust1}
Um die test cases lösen zu können, muss der Prädiktionshorizont $T=1$ gewählt werden. Die Optimierungsvariable beschränkt sich damit auf
\begin{equation*}
  z=\left( u(t), x(t+T) \right)\in\mathbb{R}^{(m+n)}, \quad T=1
\end{equation*}
Die strukturierten Matrizen im Algorithmus zum lösen des Optimierungsproblems
\begin{equation*}\begin{split}
  \min \quad &z^{T}Hz+g^{T}z\\
  \text{s.t.} \quad &Pz\leq h, \quad Cz = b 
\end{split}\end{equation*}
reduzieren sich damit auf folgende Form:
\begin{equation*}\begin{split}
  H&=\begin{bmatrix}
  R & 0\\ 
  0 & Q_{f}
  \end{bmatrix}\\
  P&=\begin{bmatrix}
  F_{u} & 0\\ 
  0 & F_{f}
  \end{bmatrix}\\
  C&=\begin{bmatrix}
  -B & I
  \end{bmatrix}\\
  g&=\begin{bmatrix}
  r+2S^{T}x(t)\\ 
  q
  \end{bmatrix}\\
  h&=\begin{bmatrix}
  f-F_{x}x(t)\\ 
  f_{f}
  \end{bmatrix}\\
  b&=\begin{bmatrix}
  Ax(t)
  \end{bmatrix}\\
\end{split}\end{equation*}
Um den Algorithmus nun mit den test cases nach \cite{MAR97} zu testen muss
\begin{equation}
  H=\frac{1}{2}\hat{Q},\quad g=\hat{c}, \text{nicht korrekt, einzelne Untermatrizen setzen}
\end{equation}
gesetzt werden. Die Ungleichungsnebenbedingung
\begin{equation}
  F_{u}u(t)+F_{x}x(t)+F_{f}x(t+1) \leq f = f_{u}+f_{x}
\end{equation}
ergeben sich zu
\begin{equation*}\begin{split}
  F_{u}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{x}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{f}&=\begin{bmatrix}
  0 & 0
  \end{bmatrix}\\
  f&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}\\
  f_{f}&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}
\end{split}\end{equation*}
Als Gleichungsnebenbedingungen bleibt im implementierten Algorithmus
\begin{equation}
  x(t+1) = Ax(t)+Bu(t)
\end{equation}
Da allerdings x(t+1) auch zu dem Vektor der Optimierungsvariablen gehört muss
\begin{equation}
  \hat{b} = -Ax(t)\quad \text{mit} \quad A=-I
\end{equation}
gesetzt werden. Zusätzlich wird mit weiteren Ungleichungsnebenbedingung dafür gesorgt, dass $x(t+1)$ im Optimum nahe der Null liegt. Das bedeutet aber auch, dass in der Auswertung der Güte der eingehaltenen Gleichungsnebenbedingungen auch die Genauigkeit der zusätzlichen Ungleichungsnebenbedingung betrachtet werden muss.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%