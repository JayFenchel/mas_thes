%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beschreibung des Algorithmus und nötigen Anppassungen %%%%%%%%%%%%%%%%%%%%%%
\chapter{Algorithmus}
\label{chap:algorithmus}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Primal Barrier Interior-Point Methode}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:PrimalBarrierInteriorPointMethode}
Das Optimierungsproblem, welches den Kern der Reglung mittles MPC bildet, muss zu jedem Zeitschritt online gelöst werden. Dabei ist es wichtig, dass eine ausreichend genaue Lösung möglichst schnell und zuverlässig gefunden werden kann. In dieser Arbeit wurde dazu eine Interior-Point Methode genutzt, bei der die Ungleichungsnebenbedingung näherungsweise durch Zuhilfenahme von logaritmische Straftermen (im folgenden als log barrier bezeichnet) berücksichtigt werden. Diese Methode wird als Primal Barrier Interior-Point Methode bezeichnet. Im Speziellen wurde hier der Ansatz von \cite{wang2010fast} verfolgt, bei dem besondere Rücksicht auf die Ausnutzung der spezifischen Struktur, wie sie bei MPC-Problemen auftritt, gelegt wird. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimierungsproblem für Fast-MPC Algorithmus}%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:OPforFastMPC}
Der Primal Barrier Interior-Point Methode nach \cite{wang2010fast} oder kurz Fast MPC
% TODO ABkürzung
auf die sich später erklärte Erweitungen beziehen liegt folgende Problembeschreibung zu Grunde.\\
Beschreibung des allgemeinen Problems das von Boyd gelöst wird.
Der Prädikitonshorizont $T$
\\
Damit ergibt sich Optimierungsproblem für die MPC zu
\begin{equation}
 \text{min}
\end{equation}
mit den Optimierungsvariablen $x(t+1), \dots, x(t+T)$ und $u(t), \dots, u(t+T-1)$. Dieses Problem lässt sich kompakt auch als
\begin{equation}
 \text{min}
\end{equation}
formulieren, worauf sich weitere Erklärungen zur Lösung dieses Optimierungsproblems beziehen.
\abk{Fast MPC}{Primal Barrier Interior-Point Methode nach \cite{wang2010fast}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fast-MPC Algorithmus}%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:FastMPC}
Wie löst Boyd sein QP?\\
Wie schon oben erwähnt werden die Ungleichungsnebenbedingungen durch Zuhilfenahme von log barriers berücksichtigt. Dadurch ergibt sich das einfacher zu lösendes Optimierungsproblem
\begin{equation}
  \text{min}
\end{equation}
mit $\kappa$ und $\Phi$, bei dem keine direkten Ungleichungsnebenbedingungen behandeln werden müssen.\\
$z$ muss immer strikt
\begin{equation}
 \label{eq:IEQ}
 Pz \leq h
\end{equation}
erfüllen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erweiterung für nichtlineare Constraints}%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungNonlincon}
Um mit dem effizienten Algorithmus aus \cite{wang2010fast} auch kompliziertere/komplexere [TODO] Probleme, in Kapitel \ref{sec:ErweiterungSOCP} Second Order Cone Problems (SOCP\abk{SOCP}{Second Order Cone Problem}), bzw in Kapitel \ref{sec:ErweiterungQCQP} Quadratic Programs/Problems mit Quadratic Constraints (QCQP\abk{QCQP}{Quadratic Constrained Quadratic Program}) zu lösen, wurde der Algorithmus wie gleich folgt erweitert. Dabei wurde speziell darauf geachtet nicht die Struktur der entstehenden Matrizen zu verändern, sodass diese auch weiterhin ausgenutzt werden kann. Allerdings sind für ein SOCP bzw QCQP nun die Matrizen für die Ungleichungsnebenbedingungen nicht mehr konstant sondern hängen von $z(k)$ ab, sodass sie in jedem MPC Schritt angepasst werden müssen, was einen erhöhten Rechenaufwand bedeutet.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Erweiterung für Quadratic Problems mit quadratic Constraints}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungQCQP}
Einführendes zu QCQPs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{QCQP-Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:QCQPFormulierung}
Zusäliche Ungleichungsnebenbedingung sieht wie folgt aus:
\begin{equation}
 x^{T}\Gamma x+\beta^{T}x\leq\alpha
\end{equation}
Mit $x=z$ kommen somit zu den $lT+l_{f}$ ursprünglichen mit den linearen Ungleichungsnebenbedingungen assoziierten Funktionen zusätzliche $p$ Funktionen für die logarithmic barrier function hinzu
\begin{equation}
 -f_{j}(z) = \alpha_{j} - z^{T}\Gamma_{j} z - \beta_{j}^{T}z, \quad j=1\dots p
\end{equation}
Für den Algorithmus wird nun weiterhin die Ableitung (Gradient und Hessian) der logarithmic barrier function $\phi(z)$ benötigt, die aus unter anderem $\nabla f_{k}(z)$ und $\nabla^{2} f_{k}(z)$ gebildet werden.
\begin{equation}
 \nabla f_{j}\left (z  \right )=2z^{T}\Gamma_{j}+\beta_{j}^{T}
\end{equation}
\begin{equation}
 \nabla^{2} f_{j}\left (z  \right )=2\Gamma_{j}
\end{equation}
Daraus ergibt sich der Gradient der barrier function $\phi(z)$ zu
\begin{equation}
 \nabla\phi\left ( z \right )=\sum_{k=1}^{lT+f_{f}+p}
 \frac{1}{\begin{bmatrix} h_{i}\\\alpha_{j} \end{bmatrix}_{k} -\begin{bmatrix} p_{i}^{T}\\\beta_{j}^{T}+z^{T}\Gamma_{j} \end{bmatrix}_{k}z}
 \begin{bmatrix} p_{i}^{T}\\\beta_{j}^{T}+2z^{T}\Gamma_{j} \end{bmatrix}_{k}
\end{equation}
Mit
\begin{equation}
 \hat{P}(z)=\begin{bmatrix} P\\ \beta_{1}^{T}+z^{T}\Gamma_{1}\\ \vdots\\ \beta_{p}^{T}+z^{T}\Gamma_{p} \end{bmatrix},\quad \hat{h} = \begin{bmatrix} h\\ \alpha_{1}^{T}\\ \vdots\\ \alpha_{p}^{T}\end{bmatrix}
\end{equation}
lässt sich das auch einfacher schreiben:
\begin{equation}
	\nabla\phi\left ( z \right )=\hat{P}^{T}(2z)\hat{d}
\end{equation}
Wobei
\begin{equation}
	\hat{d}_{k}=\frac{1}{\hat{h}_{k}-\hat{p}_{k}(z)z}
\end{equation}
und $\hat{p}_{k}(2z)$ die Zeilen in $\hat{P}(2z)$ sein sollen.
Um $\Phi$ zu bilden wird noch die zweite Ableitung von der barrier function benötigt
\begin{equation}
	\nabla^{2}\phi(z)=\hat{P}(2z)\text{diag}(\hat{d})^{2}\hat{P}(2z)+\sum_{j=1}^{p}\left (\hat{d}_{(lT+l_{f})+j}2\Gamma_{j}  \right )
\end{equation}
Die Berechnungen der Ableitungen haben also analog Struktur wie \cite{wang2010fast} bis auf den zusätzlichen Term, der bei der Hessian hinzukommt. Aber da dieser Term in der Implementierung nicht durch die Multiplikation einer großen Matrix $Gamma$ mit $z$ sonder identischer Teilmatrizen $Gamma_{k}$ mit den $T$ $x_{k}$ ist sicher gestellt das keine größeren Blöcke als $n\times n$ an den richtigen Stellen erzeugt werden. Das bedeutet, dass auch hier die Struktur der Matrix $\Phi$ nicht verändert wird.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Erweiterung für Second Order Cone Problems}%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErweiterungSOCP}
Bei der Ungleichungsnebenbedingungen für die Second Order Cone Constraints lässt sich die Berechnung leider nicht so Überschaubar darstellen, da hier Funktion und Ableitung nicht mehr so schön ähnlich sind. Die Anpassungen müssen daher wie folgt aussehen.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{SOCP Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:SOCPFormulierung}
Zusäliche Ungleichungsnebenbedingung sieht wie folgt aus [TODO: woher \cite{boyd2004convex}]:
\begin{equation}
 \label{eq:SOCPUNB}
 \left \|Ax+b  \right \|_{2} \leq c^{T}x+d
\end{equation}
Als generalized inequality nimmt Gleichung \ref{eq:SOCPUNB} leicht andere Form an:
\begin{equation}
 \left \|Ax+b  \right \|_{2}^2 \leq \left (c^{T}x+d  \right )^{2}
\end{equation}
Im folgenden lässt sich die Ungleichungsnebenbedingung so leichter umformen und ableiten, speziell hebt sich so später ein Wurzelterm auf. Mit $x=z$ ergeben sich die zusätzlichen $j$ Funktionen für die logarithmic barrier function somit zu
\begin{equation}
 -f_{j}\left (x  \right )=\left (c_{j}^{T}x+d_{j} \right )^{2}-\left \|A_{j}x+b_{j}  \right \|_{2}^2
\end{equation}
Alle $k$ barrier function Funktionen lassen sich zu
\begin{equation}
 -f_{k}\left (z  \right )
 = -\begin{bmatrix}f_{i}\\f_{j}\end{bmatrix}_{k}
 =\begin{bmatrix}h_{i}\\0\end{bmatrix}_{k} - \begin{bmatrix}p_{i}^{T}z\\
 \left (\left\|A_{j}z+b_{j}\right \|_{2}^{2} - \left(c_{j}^{T}z+d_{j}\right )^{2} \right ) \end{bmatrix}_{k}
\end{equation}
zusammenfassen. Dabei lässt sich $f_{j}$ auch noch auflösen zu
\begin{equation}
	f_{j}(z)=d_{j}^{2}-b_{j}^{T}b_{j}-\left  [-\left( c_{j}^{T}z +2d_{j} \right )c_{j}^{T}+\left ( z^{T}A_{j}^{T}+2b_{j}^{T} \right )A_{j}  \right ]z
\end{equation}
Für den Algorithmus wird nun weiterhin die Ableitung (Gradient und Hessian) der logarithmic barrier function $\phi(z)$ benötigt, die aus unter anderem $\nabla f_{k}(z)$ und $\nabla^{2} f_{k}(z)$ gebildet werden.
\begin{equation}
 \nabla f_{k}\left (z  \right )=\begin{bmatrix}p_{i}^{T}\\ \left (-2\left(\left(c_{j}^{T}z + d_{j}\right )c_{j} - A_{j}^{T} \left(A_{j}z + b_{j} \right )\right) \right)^{T}\end{bmatrix}_{k}
\end{equation}
Das Transponieren der unteren Zeile ergibt sich dadurch, dass man zu $P$ eine Spalte anhängt also zu $P^{T}$ die transformierte dieser Spalte. Damit lässt sich auch hier $\nabla f_{k}(z)$ wie schon bei die quadratic constraint schreiben.
\begin{equation}
	\nabla\phi\left ( z \right )=\hat{P}^{T}(2z)\hat{d}
\end{equation}
Mit
\begin{equation}
 \nabla^{2} f_{k}\left (z  \right )=\begin{bmatrix}0\\ -2 \left( c_{j}c_{j}^{T} - A_{j}^{T} A_{j}  \right)\end{bmatrix}_{k}
\end{equation}
ergibt sich $\nabla^{2} \phi(z)$ zu
\begin{equation}
	\nabla^{2}\phi(z)=\hat{P}(2z)\text{diag}(\hat{d})^{2}\hat{P}(2z)+\sum_{j=1}^{p}\left (\hat{d}_{(lT+l_{f})+j}-2 \left( c_{j}c_{j}^{T} - A_{j}^{T} A_{j}  \right)  \right )
\end{equation}
[TODO richtiger Index für d]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Komplette Formulierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:KompletteFormulierung}
Ungleichungsnebenbedingungen
\begin{equation}
	\hat{P}(z)z\leq \hat{h}
\end{equation}
mit
\begin{equation}
	\hat{P}(z)=\begin{bmatrix} P\\ \beta_{1}^{T}+z^{T}\Gamma_{1}\\ \vdots\\ \beta_{p}^{T}+z^{T}\Gamma_{p}\\
	-\left( c_{1}^{T}z +2d_{1} \right )c_{1}^{T}+\left ( z^{T}A_{1}^{T}+2b_{1}^{T} \right )A_{1}\\
	\vdots\\
	-\left( c_{q}^{T}z +2d_{q} \right )c_{q}^{T}+\left ( z^{T}A_{q}^{T}+2b_{q}^{T} \right )A_{q}
  \end{bmatrix}
	,\quad \hat{h} = \begin{bmatrix} h\\ \alpha_{1}^{T}\\ \vdots\\ \alpha_{p}^{T}\\d_{1}^{2}-b_{1}^{T}b_{1}\\\vdots\\d_{q}^{2}-b_{q}^{T}b_{q}\end{bmatrix}
\end{equation}
Daraus ergibt sich die Logarithmic barrier function zu
\begin{equation}
	\phi(z)=\sum_{i=1}^{lT+l_{f}+p+q}-\log \left ( \hat{h}_{i}-\hat{p}_{i}^T(z)z \right )
\end{equation}
Wobei $\hat{p}_{i}^T(z)$ die Zeilen aus $\hat{P}(z)$ sind.\\
Bei der Berechnung des Residuums wird der Gradient der logarithmic barrier function benötigt
\begin{equation}
	\nabla\phi(z)=\hat{P}^T(2z)\hat{d}
\end{equation}
mit
\begin{equation}
	\hat{d}_i=\frac{1}{\hat{h}_{i}-\hat{p}_{i}^T(z)z}
\end{equation}
Dabei muss $\hat{P}^T(z)$ mit $2z$ aufgerufen werden wie oben beschrieben.\\
Hessian der der Logarithmic barrier function für Berechnung des $\Phi$
\begin{equation}\begin{split}
	\nabla^{2}\phi(z)=&\hat{P}(2z)\text{diag}(\hat{d})^{2}\hat{P}(2z)\\
	&+\sum_{i=lT+lf+1}^{lT+lf+p}\left (\hat{d}_{i}2\Gamma_{i}  \right )
	+\sum_{j=lT+lf+p+1}^{lT+lf+p+q}\left (-2\hat{d}_{j} \left( c_{j}c_{j}^{T} - A_{j}^{T} A_{j}  \right)  \right )
\end{split}\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erweiterungen um Soft Constraints}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ErwSoftConstraints}
Was Soft Constraints sind wurde bereits beschrieben. Im benutzten Algorithmus lassen sie sich wie folgt ohne Probleme verwenden.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Selecting Kappa}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:SelectingKappa}
If $\kappa$ convergeres to zero, the gap between these minimizers does, too. Therefore in \cite{boyd2004convex} it is proposed to decrease $\kappa$ by factor 10 to 20 in every inner optimization step.\\
Additionally we bound $\kappa$ to a minimum value $\kappa_{min}>0$ to avoid nummerical problems for values of the cost term $z^{T}Mz + g^{T}z \leq 0$. Choosing $\kappa$ with this approach leads to only small changes in its value for every execution of the MPC, so the warm start still works fine. Since $\kappa$ only converges against $\kappa_{min}$ after a few MPC executions, the gap between the minimizers of \eqref{qp_for_ipm} and \eqref{cp_for_ipm} never disappears. Nevertheless with this trade-off we find sufficiently exact solutions in much less inner iterations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Anpassung für test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:testcases}
Der implementierte Algorithmus wie in [Paper:, Section:] beschrieben kann auch verwendet werden, um Optimierungsprobleme zu lösen, die ihren Ursprung nicht in der Anwendung von MPC haben. Dazu sind keine wirklichen Anpassungen des Algorithmus notwendig. Da der Algorithmus allerdings die Struktur der bei MPC auftretenden Matrizen ausnutzt, muss der jeweilige test case so ``transformiert'' werden, dass dieser eine ähnliche Struktur aufweist.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Allgemeine Beschreibung der test cases}%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:beschreibungtestcases}
Nach \cite{maros1999repository} haben die test cases folgende Form:
\begin{equation}\begin{split}
  \label{eq:mar}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{A} \hat{x} = \hat{b}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Aber es existieren auch test cases mit weiteren Ungleichungsnebenbedingung der Form:
\begin{equation}
  \label{eq:mar_unb}
  \hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}
\end{equation}
Vereinheitlicht für \ref{eq:mar} und \ref{eq:mar_unb} schreiben
\begin{equation}\begin{split}
  \min \quad &\hat{c}^{T}\hat{x}+\frac{1}{2}\hat{x}^{T}\hat{Q}\hat{x}\\
  \text{s.t.} \quad &\hat{b}_{lower} \leq \hat{A} \hat{x} \leq \hat{b}_{upper}\\
  \quad &\hat{l} \leq \hat{x} \leq \hat{u} 
\end{split}\end{equation}
Wobei sich für 
\begin{equation*}
  \hat{b} = \hat{b}_{lower} = \hat{b}_{upper}
\end{equation*}
die Gleichungsnebenbedingungen
\begin{equation*}
  \hat{A} \hat{x} = \hat{b}
\end{equation*}
ergeben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithmus mit Prädiktionshorizont gleich eins}%%%%%%%%%%%%%%%%%%
\label{sec:algorithust1}
Um die test cases lösen zu können, muss der Prädiktionshorizont $T=1$ gewählt werden. Die Optimierungsvariable beschränkt sich damit auf
\begin{equation*}
  z=\left( u(t), x(t+T) \right)\in\mathbb{R}^{(m+n)}, \quad T=1
\end{equation*}
Die strukturierten Matrizen im Algorithmus zum lösen des Optimierungsproblems
\begin{equation*}\begin{split}
  \min \quad &z^{T}Hz+g^{T}z\\
  \text{s.t.} \quad &Pz\leq h, \quad Cz = b 
\end{split}\end{equation*}
reduzieren sich damit auf folgende Form:
\begin{equation*}\begin{split}
  H&=\begin{bmatrix}
  R & 0\\ 
  0 & Q_{f}
  \end{bmatrix}\\
  P&=\begin{bmatrix}
  F_{u} & 0\\ 
  0 & F_{f}
  \end{bmatrix}\\
  C&=\begin{bmatrix}
  -B & I
  \end{bmatrix}\\
  g&=\begin{bmatrix}
  r+2S^{T}x(t)\\ 
  q
  \end{bmatrix}\\
  h&=\begin{bmatrix}
  f-F_{x}x(t)\\ 
  f_{f}
  \end{bmatrix}\\
  b&=\begin{bmatrix}
  Ax(t)
  \end{bmatrix}\\
\end{split}\end{equation*}
Um den Algorithmus nun mit den test cases nach \cite{maros1999repository} zu testen muss
\begin{equation}
  H=\frac{1}{2}\hat{Q},\quad g=\hat{c}, \text{nicht korrekt, einzelne Untermatrizen setzen}
\end{equation}
gesetzt werden. Die Ungleichungsnebenbedingung
\begin{equation}
  F_{u}u(t)+F_{x}x(t)+F_{f}x(t+1) \leq f = f_{u}+f_{x}
\end{equation}
ergeben sich zu
\begin{equation*}\begin{split}
  F_{u}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{x}&=\begin{bmatrix}
  0 & 0\\ 
  0 & 0
  \end{bmatrix}\\
  F_{f}&=\begin{bmatrix}
  0 & 0
  \end{bmatrix}\\
  f&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}\\
  f_{f}&=\begin{bmatrix}
  0\\ 
  0
  \end{bmatrix}
\end{split}\end{equation*}
Als Gleichungsnebenbedingungen bleibt im implementierten Algorithmus
\begin{equation}
  x(t+1) = Ax(t)+Bu(t)
\end{equation}
Da allerdings x(t+1) auch zu dem Vektor der Optimierungsvariablen gehört muss
\begin{equation}
  \hat{b} = -Ax(t)\quad \text{mit} \quad A=-I
\end{equation}
gesetzt werden. Zusätzlich wird mit weiteren Ungleichungsnebenbedingung dafür gesorgt, dass $x(t+1)$ im Optimum nahe der Null liegt. Das bedeutet aber auch, dass in der Auswertung der Güte der eingehaltenen Gleichungsnebenbedingungen auch die Genauigkeit der zusätzlichen Ungleichungsnebenbedingung betrachtet werden muss.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%